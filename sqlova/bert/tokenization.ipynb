{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tokenization.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"YofdPDtXxaxw","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"Tokenization classes.\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import collections\n","import unicodedata\n","import six\n","\n","\n","def convert_to_unicode(text):\n","    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        elif isinstance(text, unicode):\n","            return text\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def printable_text(text):\n","    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n","\n","    # These functions want `str` for both Python2 and Python3, but in one case\n","    # it's a Unicode string and in the other it's a byte string.\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, unicode):\n","            return text.encode(\"utf-8\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    index = 0\n","    with open(vocab_file, \"r\") as reader:\n","        while True:\n","            token = convert_to_unicode(reader.readline())\n","            if not token:\n","                break\n","            token = token.strip()\n","            vocab[token] = index\n","            index += 1\n","    return vocab\n","\n","\n","def convert_tokens_to_ids(vocab, tokens):\n","    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n","    ids = []\n","    for token in tokens:\n","        ids.append(vocab[token])\n","    return ids\n","\n","\n","def whitespace_tokenize(text):\n","    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n","    text = text.strip()\n","    if not text:\n","        return []\n","    tokens = text.split()\n","    return tokens\n","\n","\n","class FullTokenizer(object):\n","    \"\"\"Runs end-to-end tokenziation.\"\"\"\n","\n","    def __init__(self, vocab_file, do_lower_case=True):\n","        self.vocab = load_vocab(vocab_file)\n","        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n","\n","    def tokenize(self, text):\n","        split_tokens = []\n","        for token in self.basic_tokenizer.tokenize(text):\n","            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n","                split_tokens.append(sub_token)\n","\n","        return split_tokens\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        return convert_tokens_to_ids(self.vocab, tokens)\n","\n","\n","class BasicTokenizer(object):\n","    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n","\n","    def __init__(self, do_lower_case=True):\n","        \"\"\"Constructs a BasicTokenizer.\n","\n","        Args:\n","          do_lower_case: Whether to lower case the input.\n","        \"\"\"\n","        self.do_lower_case = do_lower_case\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text.\"\"\"\n","        text = convert_to_unicode(text)\n","        text = self._clean_text(text)\n","        # This was added on November 1st, 2018 for the multilingual and Chinese\n","        # models. This is also applied to the English models now, but it doesn't\n","        # matter since the English models were not trained on any Chinese data\n","        # and generally don't have any Chinese data in them (there are Chinese\n","        # characters in the vocabulary because Wikipedia does have some Chinese\n","        # words in the English Wikipedia.).\n","        text = self._tokenize_chinese_chars(text)\n","        orig_tokens = whitespace_tokenize(text)\n","        split_tokens = []\n","        for token in orig_tokens:\n","            if self.do_lower_case:\n","                token = token.lower()\n","                token = self._run_strip_accents(token)\n","            split_tokens.extend(self._run_split_on_punc(token))\n","\n","        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n","        return output_tokens\n","\n","    def _run_strip_accents(self, text):\n","        \"\"\"Strips accents from a piece of text.\"\"\"\n","        text = unicodedata.normalize(\"NFD\", text)\n","        output = []\n","        for char in text:\n","            cat = unicodedata.category(char)\n","            if cat == \"Mn\":\n","                continue\n","            output.append(char)\n","        return \"\".join(output)\n","\n","    def _run_split_on_punc(self, text):\n","        \"\"\"Splits punctuation on a piece of text.\"\"\"\n","        chars = list(text)\n","        i = 0\n","        start_new_word = True\n","        output = []\n","        while i < len(chars):\n","            char = chars[i]\n","            if _is_punctuation(char):\n","                output.append([char])\n","                start_new_word = True\n","            else:\n","                if start_new_word:\n","                    output.append([])\n","                start_new_word = False\n","                output[-1].append(char)\n","            i += 1\n","\n","        return [\"\".join(x) for x in output]\n","    \n","    def _tokenize_chinese_chars(self, text):\n","        \"\"\"Adds whitespace around any CJK character.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if self._is_chinese_char(cp):\n","                output.append(\" \")\n","                output.append(char)\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","    def _is_chinese_char(self, cp):\n","        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n","        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n","        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n","        #\n","        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n","        # despite its name. The modern Korean Hangul alphabet is a different block,\n","        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n","        # space-separated words, so they are not treated specially and handled\n","        # like the all of the other languages.\n","        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n","            (cp >= 0x3400 and cp <= 0x4DBF) or  #\n","            (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n","            (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n","            (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n","            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n","            (cp >= 0xF900 and cp <= 0xFAFF) or  #\n","            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n","            return True\n","    \n","        return False\n","    \n","    def _clean_text(self, text):\n","        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if cp == 0 or cp == 0xfffd or _is_control(char):\n","                continue\n","            if _is_whitespace(char):\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","\n","class WordpieceTokenizer(object):\n","    \"\"\"Runs WordPiece tokenization.\"\"\"\n","\n","    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n","        self.vocab = vocab\n","        self.unk_token = unk_token\n","        self.max_input_chars_per_word = max_input_chars_per_word\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text into its word pieces.\n","\n","        This uses a greedy longest-match-first algorithm to perform tokenization\n","        using the given vocabulary.\n","\n","        For example:\n","          input = \"unaffable\"\n","          output = [\"un\", \"##aff\", \"##able\"]\n","\n","        Args:\n","          text: A single token or whitespace separated tokens. This should have\n","            already been passed through `BasicTokenizer.\n","\n","        Returns:\n","          A list of wordpiece tokens.\n","        \"\"\"\n","\n","        text = convert_to_unicode(text)\n","\n","        output_tokens = []\n","        for token in whitespace_tokenize(text):\n","            chars = list(token)\n","            if len(chars) > self.max_input_chars_per_word:\n","                output_tokens.append(self.unk_token)\n","                continue\n","\n","            is_bad = False\n","            start = 0\n","            sub_tokens = []\n","            while start < len(chars):\n","                end = len(chars)\n","                cur_substr = None\n","                while start < end:\n","                    substr = \"\".join(chars[start:end])\n","                    if start > 0:\n","                        substr = \"##\" + substr\n","                    if substr in self.vocab:\n","                        cur_substr = substr\n","                        break\n","                    end -= 1\n","                if cur_substr is None:\n","                    is_bad = True\n","                    break\n","                sub_tokens.append(cur_substr)\n","                start = end\n","\n","            if is_bad:\n","                output_tokens.append(self.unk_token)\n","            else:\n","                output_tokens.extend(sub_tokens)\n","        return output_tokens\n","\n","\n","def _is_whitespace(char):\n","    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n","    # \\t, \\n, and \\r are technically contorl characters but we treat them\n","    # as whitespace since they are generally considered as such.\n","    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat == \"Zs\":\n","        return True\n","    return False\n","\n","\n","def _is_control(char):\n","    \"\"\"Checks whether `chars` is a control character.\"\"\"\n","    # These are technically control characters but we count them as whitespace\n","    # characters.\n","    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return False\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"C\"):\n","        return True\n","    return False\n","\n","\n","def _is_punctuation(char):\n","    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n","    cp = ord(char)\n","    # We treat all non-letter/number ASCII as punctuation.\n","    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n","    # Punctuation class but we treat them as punctuation anyways, for\n","    # consistency.\n","    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n","            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tbS8OtuOxzxv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}